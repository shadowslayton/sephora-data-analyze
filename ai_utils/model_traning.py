import os
from sklearn.model_selection import GridSearchCV, StratifiedKFold, ParameterGrid
from sklearn.inspection import permutation_importance
import plotly.figure_factory as ff
import plotly.graph_objects as go
from sklearn.metrics import (
    classification_report, confusion_matrix, roc_curve,
    f1_score, roc_auc_score, balanced_accuracy_score
)
from sklearn.model_selection import train_test_split, cross_val_score
from lightgbm import LGBMClassifier
from sklearn.preprocessing import RobustScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.base import BaseEstimator, TransformerMixin
import pickle
from sklearn.pipeline import Pipeline
import pandas as pd
import numpy as np
import warnings
warnings.simplefilter("ignore", pd.errors.PerformanceWarning)

# å…¨åŸŸåœæ­¢æ¨™èªŒ
_stop_training_flag = False


def set_stop_training_flag(stop=True):
    """è¨­å®šåœæ­¢è¨“ç·´æ¨™èªŒ"""
    global _stop_training_flag
    _stop_training_flag = stop
    if stop:
        print("[åœæ­¢æ©Ÿåˆ¶] å·²è¨­å®šåœæ­¢è¨“ç·´æ¨™èªŒ")


def is_training_stopped():
    """æª¢æŸ¥æ˜¯å¦æ‡‰è©²åœæ­¢è¨“ç·´"""
    global _stop_training_flag
    return _stop_training_flag


def reset_stop_training_flag():
    """é‡è¨­åœæ­¢è¨“ç·´æ¨™èªŒ"""
    global _stop_training_flag
    _stop_training_flag = False


class StoppableGridSearchCV:
    """å¯åœæ­¢çš„è¶…åƒæ•¸æœå°‹é¡åˆ¥"""

    def __init__(self, estimator, param_grid, scoring, cv, verbose=0, n_jobs=1):
        self.estimator = estimator
        self.param_grid = param_grid
        self.scoring = scoring
        self.cv = cv
        self.verbose = verbose
        self.n_jobs = n_jobs

        # çµæœå„²å­˜
        self.best_params_ = None
        self.best_score_ = -np.inf
        self.best_estimator_ = None
        self.cv_results_ = []
        self.total_combinations_ = 0
        self.completed_combinations_ = 0

    def fit(self, X, y):
        """åŸ·è¡Œå¯åœæ­¢çš„ç¶²æ ¼æœå°‹"""
        param_list = list(ParameterGrid(self.param_grid))
        self.total_combinations_ = len(param_list)

        print(f"é–‹å§‹å¯åœæ­¢çš„è¶…åƒæ•¸æœå°‹ï¼Œå…± {self.total_combinations_} å€‹åƒæ•¸çµ„åˆ...")

        for i, params in enumerate(param_list):
            # æ¯æ¬¡è¿­ä»£å‰æª¢æŸ¥åœæ­¢æ¨™èªŒ
            if is_training_stopped():
                print(
                    f"[åœæ­¢æ©Ÿåˆ¶] è¶…åƒæ•¸æœå°‹åœ¨ç¬¬ {i+1}/{self.total_combinations_} å€‹çµ„åˆæ™‚è¢«åœæ­¢")
                if self.best_params_ is not None:
                    print(f"[åœæ­¢æ©Ÿåˆ¶] è¿”å›ç›®å‰æœ€ä½³çµæœ (åˆ†æ•¸: {self.best_score_:.4f})")
                    return self
                else:
                    print("[åœæ­¢æ©Ÿåˆ¶] å°šæœªå®Œæˆä»»ä½•åƒæ•¸çµ„åˆï¼Œè¿”å›ç©ºçµæœ")
                    return None

            if self.verbose > 0:
                print(f"[{i+1}/{self.total_combinations_}] æ¸¬è©¦åƒæ•¸çµ„åˆ: {params}")

            # è¨­å®šåƒæ•¸ä¸¦è¨“ç·´æ¨¡å‹
            model_clone = self._clone_estimator_with_params(params)

            try:
                # åŸ·è¡Œäº¤å‰é©—è­‰
                cv_scores = cross_val_score(
                    model_clone, X, y,
                    cv=self.cv,
                    scoring=self.scoring,
                    n_jobs=1  # è¨­ç‚º1é¿å…ä¸¦è¡Œæ™‚çš„åœæ­¢æª¢æŸ¥å•é¡Œ
                )

                mean_score = np.mean(cv_scores)
                std_score = np.std(cv_scores)

                # å„²å­˜çµæœ
                self.cv_results_.append({
                    'params': params,
                    'mean_test_score': mean_score,
                    'std_test_score': std_score,
                    'cv_scores': cv_scores
                })

                # æ›´æ–°æœ€ä½³çµæœ
                if mean_score > self.best_score_:
                    self.best_score_ = mean_score
                    self.best_params_ = params.copy()
                    self.best_estimator_ = model_clone

                if self.verbose > 0:
                    print(f"   åˆ†æ•¸: {mean_score:.4f} (Â±{std_score:.4f})")
                    if mean_score > self.best_score_:
                        print(f"   ğŸ¯ æ–°çš„æœ€ä½³åˆ†æ•¸!")

            except Exception as e:
                print(f"   âŒ åƒæ•¸çµ„åˆ {params} è¨“ç·´å¤±æ•—: {str(e)}")
                continue

            self.completed_combinations_ = i + 1

            # åœ¨æ¯å€‹çµ„åˆå®Œæˆå¾Œå†æ¬¡æª¢æŸ¥åœæ­¢æ¨™èªŒ
            if is_training_stopped():
                print(
                    f"[åœæ­¢æ©Ÿåˆ¶] è¶…åƒæ•¸æœå°‹åœ¨å®Œæˆç¬¬ {i+1}/{self.total_combinations_} å€‹çµ„åˆå¾Œè¢«åœæ­¢")
                print(f"[åœæ­¢æ©Ÿåˆ¶] è¿”å›ç›®å‰æœ€ä½³çµæœ (åˆ†æ•¸: {self.best_score_:.4f})")
                return self

        print(f"âœ… è¶…åƒæ•¸æœå°‹å®Œæˆï¼Œæ¸¬è©¦äº† {self.completed_combinations_} å€‹åƒæ•¸çµ„åˆ")
        return self

    def _clone_estimator_with_params(self, params):
        """è¤‡è£½ä¼°è¨ˆå™¨ä¸¦è¨­å®šåƒæ•¸"""
        from sklearn.base import clone
        estimator_clone = clone(self.estimator)
        estimator_clone.set_params(**params)
        return estimator_clone


# å¿…è¦åƒæ•¸é…ç½®
TARGET_COLUMN = 'is_recommended'

# è³‡æ–™è™•ç†åƒæ•¸
TEST_SIZE = 0.2
RANDOM_STATE = 42
SIMILARITY_CUTOFF = 0.6
CATEGORICAL_THRESHOLD = 10  # æ•´æ•¸å‹é¡åˆ¥æ•¸é‡é–¾å€¼
SIMILARITY_MATCHES_COUNT = 1  # æ¨¡ç³ŠåŒ¹é…è¿”å›æ•¸é‡

# æ¨¡å‹åƒæ•¸
MODEL_N_ESTIMATORS = 250
MODEL_LEARNING_RATE = 0.01
MODEL_NUM_LEAVES = 60
MODEL_SCALE_POS_WEIGHT = 0.55
# æ™ºèƒ½è¨­å®š n_jobsï¼šå¦‚æœ CPU æ ¸å¿ƒæ•¸ > 4ï¼Œä½¿ç”¨ -1ï¼Œå¦å‰‡ä½¿ç”¨ max(1, CPU_COUNT-1)
MODEL_N_JOBS = -1
MODEL_VERBOSE = 0  # -1/0: éœé»˜, 1: åŸºæœ¬è³‡è¨Š, 2: è©³ç´°è³‡è¨Š

# è¶…åƒæ•¸èª¿å„ªåƒæ•¸
CV_FOLDS = 5
IMPORTANCE_N_REPEATS = 5
GRID_SEARCH_VERBOSE_BASIC = 2
GRID_SEARCH_VERBOSE_DETAILED = 3
SCORING_METRIC = 'f1_macro'        # ä¸»è¦è©•åˆ†æŒ‡æ¨™ï¼šf1_macro, roc_auc, balanced_accuracy
IMPORTANCE_SCORING = 'f1_macro'    # ç‰¹å¾µé‡è¦æ€§è©•åˆ†ï¼šå»ºè­°èˆ‡ä¸»è¦æŒ‡æ¨™ä¿æŒä¸€è‡´

# æª”æ¡ˆè·¯å¾‘åƒæ•¸
DEFAULT_TRAIN_DATA_PATH = "traning_data/train_data(top20).csv"
DEFAULT_MODEL_OUTPUT_PATH = "output_models/model_final.bin"

# è¶…åƒæ•¸æœå°‹ç¯„åœ
PARAM_GRID = {
    'model__n_estimators': [250, 300],
    'model__learning_rate': [0.01, 0.005],
    'model__num_leaves': [40, 60],
    'model__scale_pos_weight': [0.55, 0.56, 0.52],
    'model__reg_alpha': [0, 0.5, 1],
}


def validate_input_parameters(**kwargs):
    """
    é©—è­‰è¼¸å…¥åƒæ•¸çš„åˆç†æ€§

    åƒæ•¸:
        **kwargs: è¦é©—è­‰çš„åƒæ•¸å­—å…¸

    å›å‚³:
        bool: True å¦‚æœæ‰€æœ‰åƒæ•¸éƒ½æœ‰æ•ˆ
    """
    errors = []

    # é©—è­‰æ¸¬è©¦é›†æ¯”ä¾‹
    if 'test_size' in kwargs:
        test_size = kwargs['test_size']
        if not (0 < test_size < 1):
            errors.append(f"test_size å¿…é ˆåœ¨ 0 å’Œ 1 ä¹‹é–“ï¼Œä½†å¾—åˆ°: {test_size}")

    # é©—è­‰éš¨æ©Ÿç¨®å­
    if 'random_state' in kwargs:
        random_state = kwargs['random_state']
        if not isinstance(random_state, int) or random_state < 0:
            errors.append(f"random_state å¿…é ˆæ˜¯éè² æ•´æ•¸ï¼Œä½†å¾—åˆ°: {random_state}")

    # é©—è­‰æ¨¡å‹åƒæ•¸
    if 'n_estimators' in kwargs:
        n_estimators = kwargs['n_estimators']
        if not isinstance(n_estimators, int) or n_estimators <= 0:
            errors.append(f"n_estimators å¿…é ˆæ˜¯æ­£æ•´æ•¸ï¼Œä½†å¾—åˆ°: {n_estimators}")

    if 'learning_rate' in kwargs:
        learning_rate = kwargs['learning_rate']
        if not (0 < learning_rate <= 1):
            errors.append(f"learning_rate å¿…é ˆåœ¨ 0 å’Œ 1 ä¹‹é–“ï¼Œä½†å¾—åˆ°: {learning_rate}")

    if errors:
        print("âŒ åƒæ•¸é©—è­‰å¤±æ•—:")
        for error in errors:
            print(f"  - {error}")
        return False

    return True


def load_and_validate_data(data_path, feature_columns=None, target_column=None, default_target_column=TARGET_COLUMN, exclude_columns=None):
    """
    è¼‰å…¥å’Œé©—è­‰è³‡æ–™çš„é€šç”¨å‡½å¼

    åƒæ•¸:
        data_path (str): è³‡æ–™æª”æ¡ˆè·¯å¾‘
        feature_columns (list): ç‰¹å¾µæ¬„ä½åˆ—è¡¨
        target_column (str): ç›®æ¨™æ¬„ä½åç¨±
        default_target_column (str): é è¨­ç›®æ¨™æ¬„ä½åç¨±
        exclude_columns (list): è¦æ’é™¤çš„é«˜ç›¸é—œåº¦æ¬„ä½åˆ—è¡¨ï¼Œé˜²æ­¢è³‡æ–™æ´©æ¼

    å›å‚³:
        tuple: (data, feature_cols, target_col) æˆ– (None, None, None) å¦‚æœå¤±æ•—
    """
    print("é–‹å§‹è¼‰å…¥è³‡æ–™...")
    try:
        data = pd.read_csv(data_path)
        print(f"è³‡æ–™è¼‰å…¥å®Œæˆï¼Œè³‡æ–™å½¢ç‹€: {data.shape}")
    except FileNotFoundError:
        print(f"âŒ æ‰¾ä¸åˆ°æª”æ¡ˆ: {data_path}")
        return None, None, None
    except Exception as e:
        print(f"âŒ è¼‰å…¥è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None, None, None

    # è‡ªå‹•æª¢æ¸¬å’Œé©—è­‰æ¬„ä½
    feature_cols, target_col, missing_cols = detect_columns(
        data, feature_columns, target_column, default_target_column, exclude_columns)
    if feature_cols is None:
        print("âŒ æ¬„ä½æª¢æ¸¬å¤±æ•—")
        return None, None, None

    # é©—è­‰è³‡æ–™å‹æ…‹
    if not validate_data_types(data, feature_cols, target_col):
        print("âš ï¸  è³‡æ–™å‹æ…‹é©—è­‰æœ‰è­¦å‘Šï¼Œä½†ä»ç¹¼çºŒåŸ·è¡Œ...")

    return data, feature_cols, target_col


def create_model_pipeline(n_estimators=MODEL_N_ESTIMATORS,
                          learning_rate=MODEL_LEARNING_RATE,
                          num_leaves=MODEL_NUM_LEAVES,
                          scale_pos_weight=MODEL_SCALE_POS_WEIGHT,
                          random_state=RANDOM_STATE):
    """
    å»ºç«‹æ¨¡å‹ç®¡ç·šçš„é€šç”¨å‡½å¼

    åƒæ•¸:
        n_estimators (int): æ¨¹çš„æ•¸é‡
        learning_rate (float): å­¸ç¿’ç‡
        num_leaves (int): è‘‰å­ç¯€é»æ•¸
        scale_pos_weight (float): æ­£æ¨£æœ¬æ¬Šé‡
        random_state (int): éš¨æ©Ÿç¨®å­

    å›å‚³:
        Pipeline: åŒ…å«é è™•ç†å’Œæ¨¡å‹çš„ç®¡ç·š
    """
    model = LGBMClassifier(
        n_estimators=n_estimators,
        learning_rate=learning_rate,
        num_leaves=num_leaves,
        scale_pos_weight=scale_pos_weight,
        n_jobs=MODEL_N_JOBS,
        random_state=random_state,
        verbose=MODEL_VERBOSE
    )
    return Pipeline([('DataPreprocess', DataPreprocess()), ('model', model)])


def display_evaluation_metrics(y_true, y_pred, y_proba, dataset_name=""):
    """
    é¡¯ç¤ºè©•ä¼°æŒ‡æ¨™çš„é€šç”¨å‡½å¼

    åƒæ•¸:
        y_true: çœŸå¯¦æ¨™ç±¤
        y_pred: é æ¸¬æ¨™ç±¤
        y_proba: é æ¸¬æ©Ÿç‡
        dataset_name (str): è³‡æ–™é›†åç¨±
    """
    balanced_acc = balanced_accuracy_score(y_true, y_pred)
    f1_macro = f1_score(y_true, y_pred, average='macro')
    roc_auc = roc_auc_score(y_true, y_proba)

    print(f"\n=== {dataset_name}è©•ä¼°çµæœ ===")
    print(f"Balanced Accuracy: {balanced_acc:.4f}")
    print(f"F1-macro: {f1_macro:.4f}")
    print(f"ROC-AUC: {roc_auc:.4f}")
    print("\nåˆ†é¡å ±å‘Šï¼š")
    print(classification_report(y_true, y_pred))

    return {
        'balanced_accuracy': balanced_acc,
        'f1_macro': f1_macro,
        'roc_auc': roc_auc
    }


def detect_columns(data, feature_columns=None, target_column=None, default_target_column=TARGET_COLUMN, exclude_columns=None):
    """
    è‡ªå‹•æª¢æ¸¬å’Œé©—è­‰è³‡æ–™æ¬„ä½

    åƒæ•¸:
        data (DataFrame): è¼¸å…¥è³‡æ–™
        feature_columns (list): æŒ‡å®šçš„ç‰¹å¾µæ¬„ä½ï¼ŒNoneè¡¨ç¤ºè‡ªå‹•æ¨æ–·ï¼ˆä½¿ç”¨é™¤ç›®æ¨™æ¬„ä½å¤–çš„æ‰€æœ‰æ¬„ä½ï¼‰
        target_column (str): æŒ‡å®šçš„ç›®æ¨™æ¬„ä½ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨å¿…è¦çš„ç›®æ¨™æ¬„ä½
        default_target_column (str): é è¨­ç›®æ¨™æ¬„ä½åç¨±
        exclude_columns (list): è¦æ’é™¤çš„é«˜ç›¸é—œåº¦æ¬„ä½åˆ—è¡¨ï¼Œé¿å…è³‡æ–™æ´©æ¼ï¼Œå¦‚ ['rating'] (ç›¸é—œåº¦0.885)

    å›å‚³:
        tuple: (feature_columns, target_column, missing_columns)
    """
    available_columns = data.columns.tolist()

    # è¨­å®šç›®æ¨™æ¬„ä½
    if target_column is None:
        target_column = default_target_column

    # é‡è¦é˜²å‘†ï¼šæª¢æŸ¥ç›®æ¨™æ¬„ä½æ˜¯å¦è¢«åŒ…å«åœ¨æ’é™¤æ¬„ä½ä¸­
    if exclude_columns and target_column in exclude_columns:
        print(f"âŒ åƒæ•¸è¡çªéŒ¯èª¤ï¼")
        print(f"ç›®æ¨™æ¬„ä½ '{target_column}' ä¸èƒ½åŒæ™‚è¢«æŒ‡å®šç‚ºæ’é™¤æ¬„ä½")
        print(f"æ’é™¤æ¬„ä½åˆ—è¡¨: {exclude_columns}")
        print(f"è«‹å¾æ’é™¤æ¬„ä½ä¸­ç§»é™¤ç›®æ¨™æ¬„ä½ï¼Œæˆ–è€…æ›´æ”¹ç›®æ¨™æ¬„ä½")
        return None, None, None

    # è‡ªå‹•æ¨æ–·ç‰¹å¾µæ¬„ä½æˆ–ä½¿ç”¨æŒ‡å®šæ¬„ä½
    if feature_columns is None:
        # æº–å‚™è¦æ’é™¤çš„æ¬„ä½åˆ—è¡¨
        columns_to_exclude = [target_column]
        if exclude_columns:
            columns_to_exclude.extend(exclude_columns)

        # è‡ªå‹•ä½¿ç”¨æ‰€æœ‰æ¬„ä½ï¼Œæ’é™¤ç›®æ¨™æ¬„ä½å’ŒæŒ‡å®šæ’é™¤çš„æ¬„ä½
        feature_columns = [
            col for col in available_columns if col not in columns_to_exclude]
        print(f"ğŸ¤– è‡ªå‹•æ¨æ–·ç‰¹å¾µæ¬„ä½: {len(feature_columns)} å€‹æ¬„ä½")
        if exclude_columns:
            print(f"   æ’é™¤æ¬„ä½: {columns_to_exclude}")
        print(f"   ç‰¹å¾µæ¬„ä½: {feature_columns}")

    # é©—è­‰ç›®æ¨™æ¬„ä½æ˜¯å¦å­˜åœ¨
    if target_column not in available_columns:
        print(f"âŒ ç›®æ¨™æ¬„ä½æª¢æŸ¥å¤±æ•—ï¼")
        print(f"å¯ç”¨æ¬„ä½: {available_columns}")
        print(f"ç¼ºå°‘ç›®æ¨™æ¬„ä½: {target_column}")
        return None, None, [target_column]

    # æª¢æŸ¥ç¼ºå°‘çš„æ¬„ä½
    all_required_columns = feature_columns + [target_column]
    missing_columns = [
        col for col in all_required_columns if col not in available_columns]

    if missing_columns:
        print(f"âŒ è³‡æ–™æ¬„ä½æª¢æŸ¥å¤±æ•—ï¼")
        print(f"å¯ç”¨æ¬„ä½: {available_columns}")
        print(f"ç¼ºå°‘æ¬„ä½: {missing_columns}")
        print(f"éœ€è¦æ¬„ä½: {all_required_columns}")

        # å˜—è©¦æ™ºèƒ½åŒ¹é…ç›¸ä¼¼æ¬„ä½åç¨±
        suggestions = suggest_column_mapping(
            available_columns, missing_columns)
        if suggestions:
            print("\nğŸ’¡ å¯èƒ½çš„æ¬„ä½å°æ‡‰å»ºè­°:")
            for missing, suggestion in suggestions.items():
                print(f"  {missing} -> {suggestion}")

        return None, None, missing_columns

    print(f"âœ… è³‡æ–™æ¬„ä½æª¢æŸ¥é€šéï¼")
    print(f"ç‰¹å¾µæ¬„ä½: {feature_columns}")
    print(f"ç›®æ¨™æ¬„ä½: {target_column}")

    return feature_columns, target_column, []


def suggest_column_mapping(available_columns, missing_columns,
                           cutoff=SIMILARITY_CUTOFF,
                           matches_count=SIMILARITY_MATCHES_COUNT):
    """
    æ™ºèƒ½å»ºè­°æ¬„ä½å°æ‡‰

    åƒæ•¸:
        available_columns (list): å¯ç”¨çš„æ¬„ä½åˆ—è¡¨
        missing_columns (list): ç¼ºå°‘çš„æ¬„ä½åˆ—è¡¨
        cutoff (float): æ¨¡ç³ŠåŒ¹é…çš„ç›¸ä¼¼åº¦é–¾å€¼
        matches_count (int): è¿”å›çš„åŒ¹é…æ•¸é‡
    """
    import difflib
    suggestions = {}

    for missing in missing_columns:
        # ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…æ‰¾ç›¸ä¼¼çš„æ¬„ä½åç¨±
        matches = difflib.get_close_matches(
            missing, available_columns, n=matches_count, cutoff=cutoff
        )
        if matches:
            suggestions[missing] = matches[0]

    return suggestions


def validate_data_types(data, feature_columns, target_column):
    """
    é©—è­‰è³‡æ–™å‹æ…‹
    """
    issues = []

    # æª¢æŸ¥ç›®æ¨™è®Šæ•¸
    if target_column in data.columns:
        target_values = data[target_column].dropna().unique()
        if not all(val in [0, 1, True, False] for val in target_values):
            issues.append(
                f"ç›®æ¨™è®Šæ•¸ '{target_column}' ä¸æ˜¯äºŒå…ƒåˆ†é¡æ ¼å¼ (æ‡‰ç‚º 0/1 æˆ– True/False)")

    # æª¢æŸ¥ç‰¹å¾µæ¬„ä½çš„åŸºæœ¬è³‡è¨Š
    print(f"\nğŸ“Š è³‡æ–™æ¦‚è¦½:")
    print(f"è³‡æ–™å½¢ç‹€: {data.shape}")
    print(f"ç›®æ¨™è®Šæ•¸åˆ†å¸ƒ:")
    print(data[target_column].value_counts())

    if issues:
        print(f"âš ï¸  è³‡æ–™å‹æ…‹è­¦å‘Š:")
        for issue in issues:
            print(f"  - {issue}")

    return len(issues) == 0


class DataPreprocess(BaseEstimator, TransformerMixin):
    def __init__(self):
        self.scaler = {}
        self.fillna_value = {}
        self.onehotencode_value = {}
        self.field_names = []
        self.final_field_names = []

    def fit(self, X, y=None, field_names=None):
        self.__init__()
        if field_names is None:
            self.field_names = X.columns.tolist()
        else:
            self.field_names = field_names

        for fname in self.field_names:
            # è‡ªå‹•è£œç©ºå€¼
            if (X[fname].dtype == object) or (X[fname].dtype == str):  # å­—ä¸²å‹æ…‹æ¬„ä½
                self.fillna_value[fname] = X[fname].mode()[0]  # è£œçœ¾æ•¸
                # self.fillna_value[fname] = 'np.nan'
                # self.fillna_value[fname] = np.nan # ç¶­æŒç©ºå€¼
            elif X[fname].dtype == bool:  # å¸ƒæ—å‹æ…‹
                self.fillna_value[fname] = X[fname].mode()[0]  # è£œçœ¾æ•¸
            else:  # æ•¸å­—å‹æ…‹
                self.fillna_value[fname] = X[fname].median()  # è£œä¸­ä½æ•¸

            # è‡ªå‹•å°ºåº¦è½‰æ›(scaling)
            if (X[fname].dtype == object) or (X[fname].dtype == str):  # å­—ä¸²å‹æ…‹æ¬„ä½
                pass  # ä¸ç”¨è½‰æ›
            elif X[fname].dtype == bool:  # å¸ƒæ—å‹æ…‹
                pass  # ä¸ç”¨è½‰æ›
            else:  # æ•¸å­—å‹æ…‹
                vc = X[fname].value_counts()
                if X[fname].isin([0, 1]).all():  # ç•¶æ•¸å€¼åªæœ‰0è·Ÿ1
                    pass  # ä¸ç”¨è½‰æ›
                # æ˜¯å¦ç°¡å–®çš„æ•´æ•¸å‹é¡åˆ¥ä¸”æ•¸é‡å°æ–¼é–¾å€¼
                elif pd.api.types.is_integer_dtype(X[fname]) and X[fname].nunique() <= CATEGORICAL_THRESHOLD:
                    self.scaler[fname] = MinMaxScaler()
                    self.scaler[fname].fit(X[[fname]])
                else:  # å…¶ä»–çš„æ•¸å­—å‹æ…‹
                    self.scaler[fname] = RobustScaler()
                    self.scaler[fname].fit(X[[fname]])

            # è‡ªå‹•ç·¨ç¢¼
            if (X[fname].dtype == object) or (X[fname].dtype == str):  # å­—ä¸²å‹æ…‹æ¬„ä½, onehotencode
                field_value = X[fname].value_counts().index
                self.onehotencode_value[fname] = field_value
                for value in field_value:
                    fn = fname+"_"+value
                    # data[fn] = (data[fname] == value).astype('int8')
                    self.final_field_names.append(fn)
            elif X[fname].dtype == bool:  # å¸ƒæ—å‹æ…‹ è½‰æˆ0è·Ÿ1
                # data[fname] = data[fname].astype(int)
                self.final_field_names.append(fname)
            else:  # æ•¸å­—å‹æ…‹ ä¸ç”¨é‡æ–°ç·¨ç¢¼
                self.final_field_names.append(fname)

        return self

    def transform(self, X):
        # å¦‚æœè¼¸å…¥çš„dataæ˜¯dictï¼Œè¦å…ˆè½‰æˆdataframe
        if isinstance(X, dict):
            for fname in self.field_names:
                if fname in X:
                    X[fname] = [X[fname]]
                else:
                    X[fname] = [np.nan]
            data = pd.DataFrame(X)
        else:  # å°‡è³‡æ–™è¤‡è£½ä¸€ä»½ï¼Œä¸ä¿®æ”¹åŸæœ¬çš„è³‡æ–™
            data = X.copy()

        for fname in self.field_names:
            # è‡ªå‹•è£œç©ºå€¼
            if data[fname].isnull().any():  # æœ‰ç©ºå€¼
                # if fname in self.fillna_value:
                data[fname] = data[fname].fillna(self.fillna_value[fname])

            # è‡ªå‹•å°ºåº¦è½‰æ›(scaling)
            if fname in self.scaler:
                data[fname] = self.scaler[fname].transform(data[[fname]])

            # è‡ªå‹•ç·¨ç¢¼
            if (data[fname].dtype == object) or (data[fname].dtype == str):  # å­—ä¸²å‹æ…‹æ¬„ä½, onehotencode
                if fname in self.onehotencode_value:
                    field_value = self.onehotencode_value[fname]
                    for value in field_value:
                        fn = fname+"_"+value
                        data[fn] = (data[fname] == value).astype('int8')
            elif data[fname].dtype == bool:  # å¸ƒæ—å‹æ…‹ è½‰æˆ0è·Ÿ1
                data[fname] = data[fname].astype(int)
            else:  # æ•¸å­—å‹æ…‹ ä¸ç”¨é‡æ–°ç·¨ç¢¼
                pass
        return data[self.final_field_names]

    def save(self, file_name):
        with open(file_name, "wb") as f:
            pickle.dump(self, f)

    @staticmethod
    def load(file_name):
        with open(file_name, "rb") as f:
            return pickle.load(f)


def train_model(data_path=DEFAULT_TRAIN_DATA_PATH,
                output_path=DEFAULT_MODEL_OUTPUT_PATH,
                show_plots=True,
                feature_columns=None,
                target_column=None,
                default_target_column=TARGET_COLUMN,
                exclude_columns=None,
                test_size=TEST_SIZE,
                random_state=RANDOM_STATE,
                n_estimators=MODEL_N_ESTIMATORS,
                learning_rate=MODEL_LEARNING_RATE,
                num_leaves=MODEL_NUM_LEAVES,
                scale_pos_weight=MODEL_SCALE_POS_WEIGHT,
                n_repeats=IMPORTANCE_N_REPEATS,
                plot_width=600,
                plot_height=500,
                plot_height_square=600):
    """
    è¨“ç·´ Sephora ç”¢å“æ¨è–¦æ¨¡å‹

    åƒæ•¸:
        data_path (str): è¨“ç·´è³‡æ–™è·¯å¾‘
        output_path (str): æ¨¡å‹è¼¸å‡ºè·¯å¾‘
        show_plots (bool): æ˜¯å¦é¡¯ç¤ºåœ–è¡¨
        feature_columns (list): ç‰¹å¾µæ¬„ä½åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºè‡ªå‹•æ¨æ–·
        target_column (str): ç›®æ¨™æ¬„ä½åç¨±ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨é è¨­
        default_target_column (str): é è¨­ç›®æ¨™æ¬„ä½åç¨±
        exclude_columns (list): è¦æ’é™¤çš„é«˜ç›¸é—œåº¦æ¬„ä½åˆ—è¡¨ï¼Œé¿å…è³‡æ–™æ´©æ¼ï¼Œå¦‚ ['rating'] (ç›¸é—œåº¦0.885)
        test_size (float): æ¸¬è©¦é›†æ¯”ä¾‹
        random_state (int): éš¨æ©Ÿç¨®å­
        n_estimators (int): æ¨¡å‹æ¨¹çš„æ•¸é‡
        learning_rate (float): å­¸ç¿’ç‡
        num_leaves (int): è‘‰å­ç¯€é»æ•¸
        scale_pos_weight (float): æ­£æ¨£æœ¬æ¬Šé‡
        n_repeats (int): ç‰¹å¾µé‡è¦æ€§è¨ˆç®—é‡è¤‡æ¬¡æ•¸
        plot_width (int): åœ–è¡¨å¯¬åº¦
        plot_height (int): åœ–è¡¨é«˜åº¦
        plot_height_square (int): æ–¹å½¢åœ–è¡¨é«˜åº¦

    å›å‚³:
        dict: åŒ…å«æ¨¡å‹å’Œè©•ä¼°çµæœçš„å­—å…¸ï¼Œå¦‚æœè¢«åœæ­¢å‰‡å›å‚³ None
    """
    # é¦–å…ˆæª¢æŸ¥æ˜¯å¦å·²ç¶“è¢«è«‹æ±‚åœæ­¢
    if is_training_stopped():
        print("[åœæ­¢æ©Ÿåˆ¶] è¨“ç·´åœ¨é–‹å§‹å‰è¢«åœæ­¢")
        return None

    # è¼‰å…¥å’Œé©—è­‰è³‡æ–™
    data, feature_cols, target_col = load_and_validate_data(
        data_path, feature_columns, target_column, default_target_column, exclude_columns)
    if data is None or feature_cols is None or target_col is None:
        return None

    # æª¢æŸ¥åœæ­¢æ¨™èªŒ
    if is_training_stopped():
        print("[åœæ­¢æ©Ÿåˆ¶] è¨“ç·´åœ¨è³‡æ–™è¼‰å…¥å¾Œè¢«åœæ­¢")
        return None    # æº–å‚™ç‰¹å¾µå’Œç›®æ¨™è®Šæ•¸
    try:
        X = data[feature_cols]
        y = data[target_col]
        y = y.astype(int)
    except KeyError as e:
        print(f"âŒ æå–æ¬„ä½æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None
    except Exception as e:
        print(f"âŒ è³‡æ–™è™•ç†æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None

    # è¨ˆç®—é¡åˆ¥æ¬Šé‡
    num_class_1 = np.sum(y == 1)
    num_class_0 = np.sum(y == 0)
    scale_pos_weight_value = num_class_0 / num_class_1
    print(f"è¨ˆç®—å‡ºçš„ scale_pos_weight å€¼ï¼š{scale_pos_weight_value}")

    # å»ºç«‹æ¨¡å‹ç®¡ç·š
    pipe = create_model_pipeline(
        n_estimators=n_estimators,
        learning_rate=learning_rate,
        num_leaves=num_leaves,
        scale_pos_weight=scale_pos_weight,
        random_state=random_state
    )

    # åˆ†å‰²è³‡æ–™
    X_train, X_valid, y_train, y_valid = train_test_split(
        X, y,
        test_size=test_size,
        random_state=random_state,
        stratify=y)

    # æª¢æŸ¥åœæ­¢æ¨™èªŒ
    if is_training_stopped():
        print("[åœæ­¢æ©Ÿåˆ¶] è¨“ç·´åœ¨è³‡æ–™åˆ†å‰²å¾Œè¢«åœæ­¢")
        return None

    print("é–‹å§‹è¨“ç·´æ¨¡å‹...")
    print("[æ³¨æ„] æ¨¡å‹è¨“ç·´éšæ®µç„¡æ³•ä¸­é€”åœæ­¢ï¼Œè«‹ç­‰å¾…å®Œæˆ...")
    pipe.fit(X_train, y_train)
    print("æ¨¡å‹è¨“ç·´å®Œæˆ!")

    # æª¢æŸ¥åœæ­¢æ¨™èªŒ
    if is_training_stopped():
        print("[åœæ­¢æ©Ÿåˆ¶] è¨“ç·´åœ¨æ¨¡å‹è¨“ç·´å¾Œè¢«åœæ­¢")
        return None

    # è©•ä¼°æ¨¡å‹
    prediction_train = pipe.predict(X_train)
    proba_train = pipe.predict_proba(X_train)[:, 1]
    train_metrics = display_evaluation_metrics(
        y_train, prediction_train, proba_train, "è¨“ç·´çµ„")

    prediction_valid = pipe.predict(X_valid)
    proba_valid = pipe.predict_proba(X_valid)[:, 1]
    valid_metrics = display_evaluation_metrics(
        y_valid, prediction_valid, proba_valid, "é©—è­‰çµ„")

    # é¡¯ç¤ºåœ–è¡¨
    if show_plots:
        # ROC æ›²ç·š - è¨“ç·´çµ„
        fpr_train, tpr_train, _ = roc_curve(y_train, proba_train)
        roc_fig_train = go.Figure()
        roc_fig_train.add_trace(go.Scatter(
            x=fpr_train, y=tpr_train,
            mode='lines',
            name=f'ROC æ›²ç·š (AUC = {train_metrics["roc_auc"]:.3f})',
            line=dict(color='blue', width=2)
        ))
        roc_fig_train.add_trace(go.Scatter(
            x=[0, 1], y=[0, 1],
            mode='lines',
            name='éš¨æ©ŸçŒœæ¸¬',
            line=dict(color='red', dash='dash')
        ))
        roc_fig_train.update_layout(
            title='è¨“ç·´çµ„ ROC æ›²ç·š',
            xaxis_title='å½é™½æ€§ç‡ (False Positive Rate)',
            yaxis_title='çœŸé™½æ€§ç‡ (True Positive Rate)',
            width=plot_width,
            height=plot_height
        )
        roc_fig_train.show()

        # ROC æ›²ç·š - é©—è­‰çµ„
        fpr_valid, tpr_valid, _ = roc_curve(y_valid, proba_valid)
        roc_fig_valid = go.Figure()
        roc_fig_valid.add_trace(go.Scatter(
            x=fpr_valid, y=tpr_valid,
            mode='lines',
            name=f'ROC æ›²ç·š (AUC = {valid_metrics["roc_auc"]:.3f})',
            line=dict(color='green', width=2)
        ))
        roc_fig_valid.add_trace(go.Scatter(
            x=[0, 1], y=[0, 1],
            mode='lines',
            name='éš¨æ©ŸçŒœæ¸¬',
            line=dict(color='red', dash='dash')
        ))
        roc_fig_valid.update_layout(
            title='é©—è­‰çµ„ ROC æ›²ç·š',
            xaxis_title='å½é™½æ€§ç‡ (False Positive Rate)',
            yaxis_title='çœŸé™½æ€§ç‡ (True Positive Rate)',
            width=plot_width,
            height=plot_height
        )
        roc_fig_valid.show()

        # æ··æ·†çŸ©é™£
        cm = confusion_matrix(y_valid, prediction_valid)
        cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100

        labels = [[f"{num}<br>({perc:.1f}%)" for num, perc in zip(row, perc_row)]
                  for row, perc_row in zip(cm, cm_percent)]

        fig = ff.create_annotated_heatmap(
            z=cm_percent,
            x=["Predicted 0", "Predicted 1"],
            y=["True 0", "True 1"],
            annotation_text=labels,
            colorscale="Blues",
            showscale=True
        )
        fig.update_layout(
            title="é©—è­‰çµ„æ··æ·†çŸ©é™£ (Count & %)",
            xaxis_title="Predicted Label",
            yaxis_title="True Label",
            yaxis_autorange='reversed',
            width=plot_width,
            height=plot_height_square
        )
        fig.show()

    # ç”¨å…¨éƒ¨è³‡æ–™é‡æ–°è¨“ç·´æœ€çµ‚æ¨¡å‹
    print("\nç”¨å…¨éƒ¨è³‡æ–™é‡æ–°è¨“ç·´æœ€çµ‚æ¨¡å‹...")

    # æª¢æŸ¥åœæ­¢æ¨™èªŒ
    if is_training_stopped():
        print("[åœæ­¢æ©Ÿåˆ¶] è¨“ç·´åœ¨æœ€çµ‚æ¨¡å‹è¨“ç·´å‰è¢«åœæ­¢")
        return None

    print("[æ³¨æ„] æœ€çµ‚æ¨¡å‹è¨“ç·´éšæ®µç„¡æ³•ä¸­é€”åœæ­¢ï¼Œè«‹ç­‰å¾…å®Œæˆ...")
    pipe.fit(X, y)

    # æª¢æŸ¥åœæ­¢æ¨™èªŒ
    if is_training_stopped():
        print("[åœæ­¢æ©Ÿåˆ¶] è¨“ç·´åœ¨æœ€çµ‚æ¨¡å‹è¨“ç·´å¾Œè¢«åœæ­¢")
        return None

    # å„²å­˜æ¨¡å‹å’Œæ¬„ä½è³‡è¨Š
    model_info = {
        'pipeline': pipe,
        'feature_columns': feature_cols,
        'target_column': target_col
    }

    with open(output_path, "wb") as f:
        pickle.dump(model_info, f)
    print(f"æ¨¡å‹å·²å„²å­˜è‡³: {output_path}")
    print(f"åŒ…å«æ¬„ä½è³‡è¨Š: ç‰¹å¾µæ¬„ä½={len(feature_cols)}å€‹, ç›®æ¨™æ¬„ä½='{target_col}'")

    # ç‰¹å¾µé‡è¦æ€§åˆ†æ
    print("\nè¨ˆç®—ç‰¹å¾µé‡è¦æ€§...")

    # æª¢æŸ¥åœæ­¢æ¨™èªŒ
    if is_training_stopped():
        print("[åœæ­¢æ©Ÿåˆ¶] è¨“ç·´åœ¨ç‰¹å¾µé‡è¦æ€§è¨ˆç®—å‰è¢«åœæ­¢")
        return None

    print("[æ³¨æ„] ç‰¹å¾µé‡è¦æ€§è¨ˆç®—éšæ®µç„¡æ³•ä¸­é€”åœæ­¢ï¼Œè«‹ç­‰å¾…å®Œæˆ...")
    result = permutation_importance(
        pipe, X, y, scoring=IMPORTANCE_SCORING, n_repeats=n_repeats, random_state=random_state)

    importances = getattr(result, 'importances_mean')
    # ä½¿ç”¨é è™•ç†å™¨çš„æœ€çµ‚æ¬„ä½åç¨±è€Œä¸æ˜¯åŸå§‹æ¬„ä½åç¨±
    preprocessor = pipe.named_steps['DataPreprocess']
    features = preprocessor.final_field_names

    feature_importance = list(zip(features, importances))
    feature_importance_sorted = sorted(
        feature_importance, key=lambda x: x[1], reverse=True)

    print("\n=== ç‰¹å¾µé‡è¦æ€§æ’åº ===")
    for feature, importance in feature_importance_sorted:
        print(f"{feature}: {importance:.4f}")

    # å›å‚³çµæœ
    results = {
        'model': pipe,
        'feature_columns': feature_cols,
        'target_column': target_col,
        'train_metrics': train_metrics,
        'valid_metrics': valid_metrics,
        'feature_importance': feature_importance_sorted
    }

    return results


def hyperparameter_tuning(data_path=DEFAULT_TRAIN_DATA_PATH,
                          quick_mode=False,
                          feature_columns=None,
                          target_column=None,
                          default_target_column=TARGET_COLUMN,
                          test_size=TEST_SIZE,
                          random_state=RANDOM_STATE,
                          cv_folds=CV_FOLDS,
                          param_grid=None,
                          exclude_columns=None):
    """
    åŸ·è¡Œè¶…åƒæ•¸èª¿å„ª

    åƒæ•¸:
        data_path (str): è¨“ç·´è³‡æ–™è·¯å¾‘
        quick_mode (bool): å¿«é€Ÿæ¨¡å¼ï¼Œä½¿ç”¨è¼ƒå°‘åƒæ•¸çµ„åˆ
        feature_columns (list): ç‰¹å¾µæ¬„ä½åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºè‡ªå‹•æ¨æ–·
        target_column (str): ç›®æ¨™æ¬„ä½åç¨±ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨é è¨­
        default_target_column (str): é è¨­ç›®æ¨™æ¬„ä½åç¨±
        test_size (float): æ¸¬è©¦é›†æ¯”ä¾‹
        random_state (int): éš¨æ©Ÿç¨®å­
        cv_folds (int): äº¤å‰é©—è­‰æŠ˜æ•¸
        param_grid (dict): åƒæ•¸æœå°‹ç¶²æ ¼ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨é è¨­
        exclude_columns (list): è¦æ’é™¤çš„é«˜ç›¸é—œåº¦æ¬„ä½åˆ—è¡¨ï¼Œå¦‚ ['rating'] (ç›¸é—œåº¦0.885)ï¼ŒNoneè¡¨ç¤ºä¸æ’é™¤ä»»ä½•æ¬„ä½

    å›å‚³:
        dict: æœ€ä½³åƒæ•¸å’Œæ¨¡å‹ï¼Œå¦‚æœè¢«åœæ­¢å‰‡å›å‚³ None
    """
    print("é–‹å§‹è¶…åƒæ•¸èª¿å„ª...")

    # è¼‰å…¥å’Œé©—è­‰è³‡æ–™
    data, feature_cols, target_col = load_and_validate_data(
        data_path, feature_columns, target_column, default_target_column, exclude_columns)
    if data is None or feature_cols is None or target_col is None:
        return None

    # æª¢æŸ¥åœæ­¢æ¨™èªŒ
    if is_training_stopped():
        print("[åœæ­¢æ©Ÿåˆ¶] è¶…åƒæ•¸èª¿å„ªåœ¨è³‡æ–™è¼‰å…¥å¾Œè¢«åœæ­¢")
        return None

    # æº–å‚™ç‰¹å¾µå’Œç›®æ¨™è®Šæ•¸
    try:
        X = data[feature_cols]
        y = data[target_col]
        y = y.astype(int)
    except Exception as e:
        print(f"âŒ è³‡æ–™è™•ç†æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None

    X_train, X_valid, y_train, y_valid = train_test_split(
        X, y, test_size=test_size, random_state=random_state, stratify=y)

    model = LGBMClassifier(n_jobs=MODEL_N_JOBS,
                           random_state=random_state, verbose=MODEL_VERBOSE)
    pipe = Pipeline([('DataPreprocess', DataPreprocess()), ('model', model)])

    # ä½¿ç”¨å‚³å…¥çš„åƒæ•¸ç¶²æ ¼æˆ–é è¨­ç¶²æ ¼
    if param_grid is None:
        param_grid = PARAM_GRID

    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True,
                         random_state=random_state)

    # ä½¿ç”¨å¯åœæ­¢çš„ç¶²æ ¼æœå°‹
    grid_search = StoppableGridSearchCV(
        estimator=pipe,
        param_grid=param_grid,
        scoring=SCORING_METRIC,
        cv=cv,
        verbose=2,  # é¡¯ç¤ºè©³ç´°é€²åº¦
        n_jobs=1    # ä½¿ç”¨å–®åŸ·è¡Œç·’ç¢ºä¿åœæ­¢æ©Ÿåˆ¶æ­£å¸¸é‹ä½œ
    )

    # è¨ˆç®—ç¸½çµ„åˆæ•¸
    total_combinations = (len(param_grid['model__n_estimators']) *
                          len(param_grid['model__learning_rate']) *
                          len(param_grid['model__num_leaves']) *
                          len(param_grid['model__scale_pos_weight']) *
                          len(param_grid['model__reg_alpha']))
    total_fits = total_combinations * cv_folds

    print(f"\n=== è¶…åƒæ•¸èª¿å„ªé…ç½® ===")
    print(f"åƒæ•¸çµ„åˆæ•¸ï¼š{total_combinations} å€‹")
    print(f"äº¤å‰é©—è­‰ï¼š{cv_folds} fold")
    print(f"ç¸½è¨ˆç®—æ¬¡æ•¸ï¼š{total_fits} æ¬¡æ¨¡å‹è¨“ç·´")
    print(f"é–‹å§‹æ™‚é–“ï¼š{pd.Timestamp.now().strftime('%H:%M:%S')}")

    print(f"\nåƒæ•¸æœå°‹ç¯„åœï¼š")
    for param, values in param_grid.items():
        print(f"  {param}: {values}")

    print(f"\né–‹å§‹åŸ·è¡Œè¶…åƒæ•¸æœå°‹...")
    print("=" * 50)

    # æª¢æŸ¥åœæ­¢æ¨™èªŒ
    if is_training_stopped():
        print("[åœæ­¢æ©Ÿåˆ¶] è¶…åƒæ•¸èª¿å„ªåœ¨ç¶²æ ¼æœå°‹å‰è¢«åœæ­¢")
        return None

    print("âœ… ç¾åœ¨æ”¯æ´ä¸­é€”åœæ­¢è¶…åƒæ•¸æœå°‹!")
    result = grid_search.fit(X_train, y_train)

    # æª¢æŸ¥æ˜¯å¦å› åœæ­¢è€Œæå‰çµæŸ
    if result is None:
        print("[åœæ­¢æ©Ÿåˆ¶] è¶…åƒæ•¸èª¿å„ªè¢«ä½¿ç”¨è€…åœæ­¢ï¼Œæœªå®Œæˆä»»ä½•åƒæ•¸çµ„åˆ")
        return None

    # æª¢æŸ¥åœæ­¢æ¨™èªŒ
    if is_training_stopped():
        print("[åœæ­¢æ©Ÿåˆ¶] è¶…åƒæ•¸èª¿å„ªåœ¨ç¶²æ ¼æœå°‹å¾Œè¢«åœæ­¢")
        # å¦‚æœæœ‰éƒ¨åˆ†çµæœï¼Œä»ç„¶è¿”å›æœ€ä½³çµæœ
        if grid_search.best_params_ is not None:
            print(
                f"[åœæ­¢æ©Ÿåˆ¶] è¿”å›éƒ¨åˆ†æœå°‹çµæœ (å®Œæˆ {grid_search.completed_combinations_}/{grid_search.total_combinations_} å€‹çµ„åˆ)")
        else:
            return None

    print("=" * 50)
    print(f"è¶…åƒæ•¸æœå°‹å®Œæˆæ™‚é–“ï¼š{pd.Timestamp.now().strftime('%H:%M:%S')}")
    print("=" * 50)

    # ç¢ºä¿æœ‰æœ‰æ•ˆçš„çµæœ
    if grid_search.best_params_ is None or grid_search.best_estimator_ is None:
        print("âŒ è¶…åƒæ•¸æœå°‹æœªç”¢ç”Ÿæœ‰æ•ˆçµæœ")
        return None

    print("æœ€ä½³åƒæ•¸çµ„åˆ:", grid_search.best_params_)
    print("æœ€ä½³ f1_macro åˆ†æ•¸:", grid_search.best_score_)

    best_model = grid_search.best_estimator_

    # æª¢æŸ¥åœæ­¢æ¨™èªŒï¼Œå¦‚æœè¢«åœæ­¢å‰‡è·³éé©—è­‰æ­¥é©Ÿ
    if not is_training_stopped():
        y_pred = best_model.predict(X_valid)
        print("\næœ€ä½³æ¨¡å‹åœ¨é©—è­‰çµ„çš„è¡¨ç¾:")
        print(classification_report(y_valid, y_pred))
    else:
        print("\n[åœæ­¢æ©Ÿåˆ¶] è·³éæ¨¡å‹é©—è­‰æ­¥é©Ÿ")

    return {
        'best_params': grid_search.best_params_,
        'best_score': grid_search.best_score_,
        'best_model': best_model,
        'feature_columns': feature_cols,
        'target_column': target_col
    }


def load_model_with_info(model_path, default_target_column=TARGET_COLUMN):
    """
    è¼‰å…¥æ¨¡å‹å’Œæ¬„ä½è³‡è¨Š

    åƒæ•¸:
        model_path (str): æ¨¡å‹æª”æ¡ˆè·¯å¾‘
        default_target_column (str): é è¨­ç›®æ¨™æ¬„ä½åç¨±

    å›å‚³:
        dict: åŒ…å«æ¨¡å‹å’Œæ¬„ä½è³‡è¨Šçš„å­—å…¸
    """
    try:
        with open(model_path, "rb") as f:
            model_info = pickle.load(f)

        # å…¼å®¹èˆŠç‰ˆæœ¬æ¨¡å‹æª”æ¡ˆï¼ˆåªæœ‰ pipelineï¼‰
        if hasattr(model_info, 'predict'):  # é€™æ˜¯èˆŠç‰ˆæœ¬çš„ pipeline ç‰©ä»¶
            print("âš ï¸  è¼‰å…¥çš„æ˜¯èˆŠç‰ˆæœ¬æ¨¡å‹ï¼Œç¼ºå°‘æ¬„ä½è³‡è¨Šï¼Œéœ€è¦æ‰‹å‹•æŒ‡å®šç‰¹å¾µæ¬„ä½")
            return {
                'pipeline': model_info,
                'feature_columns': None,  # éœ€è¦æ‰‹å‹•æŒ‡å®š
                'target_column': default_target_column
            }
        else:  # æ–°ç‰ˆæœ¬åŒ…å«å®Œæ•´è³‡è¨Š
            print("âœ… è¼‰å…¥æ–°ç‰ˆæœ¬æ¨¡å‹ï¼ŒåŒ…å«å®Œæ•´æ¬„ä½è³‡è¨Š")
            return model_info

    except FileNotFoundError:
        print(f"âŒ æ‰¾ä¸åˆ°æ¨¡å‹æª”æ¡ˆ: {model_path}")
        return None
    except Exception as e:
        print(f"âŒ è¼‰å…¥æ¨¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None


# ä¸»ç¨‹å¼åŸ·è¡Œå€å¡Š
if __name__ == "__main__":
    print("=== Sephora ç”¢å“æ¨è–¦æ¨¡å‹è¨“ç·´ ===")

    # é¸æ“‡è¦åŸ·è¡Œçš„åŠŸèƒ½
    choice = input("è«‹é¸æ“‡åŠŸèƒ½ (1: è¨“ç·´æ¨¡å‹, 2: è¶…åƒæ•¸èª¿å„ª, 3: å…©è€…éƒ½åŸ·è¡Œ): ")

    if choice == "1":
        results = train_model()
        print("\næ¨¡å‹è¨“ç·´å®Œæˆ!")

    elif choice == "2":
        tuning_results = hyperparameter_tuning()
        print("\nè¶…åƒæ•¸èª¿å„ªå®Œæˆ!")

    elif choice == "3":
        # å…ˆåŸ·è¡Œè¶…åƒæ•¸èª¿å„ª
        tuning_results = hyperparameter_tuning()

        if tuning_results:
            print("\nç¾åœ¨ç”¨æœ€ä½³åƒæ•¸è¨“ç·´æœ€çµ‚æ¨¡å‹...")

            # æå–æœ€ä½³åƒæ•¸
            best_params = tuning_results['best_params']

            # ç”¨æœ€ä½³åƒæ•¸è¨“ç·´æ¨¡å‹
            results = train_model(
                n_estimators=best_params.get(
                    'model__n_estimators', MODEL_N_ESTIMATORS),
                learning_rate=best_params.get(
                    'model__learning_rate', MODEL_LEARNING_RATE),
                num_leaves=best_params.get(
                    'model__num_leaves', MODEL_NUM_LEAVES),
                scale_pos_weight=best_params.get(
                    'model__scale_pos_weight', MODEL_SCALE_POS_WEIGHT)
            )

            print(f"\nğŸ¯ ä½¿ç”¨çš„æœ€ä½³åƒæ•¸:")
            for param, value in best_params.items():
                clean_param = param.replace('model__', '')
                print(f"  {clean_param}: {value}")

            print("\næ‰€æœ‰è¨“ç·´å®Œæˆ!")
        else:
            print("âŒ è¶…åƒæ•¸èª¿å„ªå¤±æ•—ï¼Œç„¡æ³•ç¹¼çºŒè¨“ç·´")

    else:
        print("ç„¡æ•ˆçš„é¸æ“‡ï¼Œè«‹è¼¸å…¥ 1, 2 æˆ– 3")
