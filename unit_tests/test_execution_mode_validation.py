#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¸¬è©¦ä¸‰ç¨®åŸ·è¡Œæ¨¡å¼çš„åƒæ•¸é©—è­‰é‚è¼¯
å°ˆé–€æ¸¬è©¦æ¨¡å¼1ã€æ¨¡å¼2ã€æ¨¡å¼3çš„é©—è­‰å·®ç•°
"""

import unittest
import sys
import os

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ°è·¯å¾‘ï¼ˆå¿…é ˆåœ¨ import app_utils ä¹‹å‰ï¼‰
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

# å‹•æ…‹ importï¼Œé¿å…è¢«æ ¼å¼åŒ–å·¥å…·å¹²æ“¾
ParameterValidator = __import__('app_utils.parameter_validator', fromlist=[
                                'ParameterValidator']).ParameterValidator


class TestExecutionModeValidation(unittest.TestCase):
    """æ¸¬è©¦ä¸‰ç¨®åŸ·è¡Œæ¨¡å¼çš„åƒæ•¸é©—è­‰"""

    def setUp(self):
        """è¨­å®šæ¸¬è©¦ç’°å¢ƒ"""
        class MockVar:
            def __init__(self, value):
                self.value = value

            def get(self):
                return self.value

            def set(self, value):
                self.value = value

        # å‰µå»ºæ¨¡æ“¬æ‡‰ç”¨ç¨‹å¼
        class MockApp:
            def __init__(self, mode):
                # åŸºæœ¬åƒæ•¸
                self.run_mode = MockVar(mode)
                self.target_column = MockVar("")
                self.exclude_columns = MockVar("")
                self.train_data_path = MockVar("")
                self.model_output_folder = MockVar("")
                self.model_filename = MockVar("")

                # è¨­å®šåˆç†çš„é è¨­æ•¸å€¼åƒæ•¸
                self.test_size = MockVar(0.2)
                self.random_state = MockVar(42)
                self.similarity_cutoff = MockVar(0.6)
                self.categorical_threshold = MockVar(10)
                self.similarity_matches_count = MockVar(1)
                self.model_n_estimators = MockVar(250)
                self.model_learning_rate = MockVar(0.01)
                self.model_num_leaves = MockVar(60)
                self.model_scale_pos_weight = MockVar(0.55)
                self.model_n_jobs = MockVar(-1)
                self.model_verbose = MockVar(0)
                self.cv_folds = MockVar(5)
                self.importance_n_repeats = MockVar(5)
                self.grid_search_verbose_basic = MockVar(2)
                self.grid_search_verbose_detailed = MockVar(3)
                self.scoring_metric = MockVar('f1_macro')
                self.importance_scoring = MockVar('f1_macro')

        self.mock_apps = {
            "1": MockApp("1"),
            "2": MockApp("2"),
            "3": MockApp("3")
        }

    def test_mode1_required_parameters(self):
        """æ¸¬è©¦æ¨¡å¼1çš„å¿…å¡«åƒæ•¸é©—è­‰"""
        print("=== æ¸¬è©¦æ¨¡å¼1å¿…å¡«åƒæ•¸é©—è­‰ ===")

        app = self.mock_apps["1"]
        validator = ParameterValidator(app)

        # æ¸¬è©¦ç©ºåƒæ•¸
        errors = validator.validate_all_parameters()
        required_errors = [e for e in errors if "å¿…å¡«é …ç›®" in e]

        self.assertEqual(len(required_errors), 4, "æ¨¡å¼1æ‡‰è©²æœ‰4å€‹å¿…å¡«é …ç›®éŒ¯èª¤")

        # æª¢æŸ¥å…·é«”çš„å¿…å¡«é …ç›®
        error_text = "\n".join(required_errors)
        self.assertIn("ç›®æ¨™æ¬„ä½", error_text)
        self.assertIn("è¨“ç·´è³‡æ–™æª”æ¡ˆ", error_text)
        self.assertIn("æ¨¡å‹è¼¸å‡ºè³‡æ–™å¤¾", error_text)
        self.assertIn("æ¨¡å‹æª”æ¡ˆåç¨±", error_text)

        print("âœ… æ¨¡å¼1å¿…å¡«åƒæ•¸é©—è­‰æ­£ç¢º")

    def test_mode2_required_parameters(self):
        """æ¸¬è©¦æ¨¡å¼2çš„å¿…å¡«åƒæ•¸é©—è­‰"""
        print("=== æ¸¬è©¦æ¨¡å¼2å¿…å¡«åƒæ•¸é©—è­‰ ===")

        app = self.mock_apps["2"]
        validator = ParameterValidator(app)

        # æ¸¬è©¦ç©ºåƒæ•¸
        errors = validator.validate_all_parameters()
        required_errors = [e for e in errors if "å¿…å¡«é …ç›®" in e]

        self.assertEqual(len(required_errors), 2, "æ¨¡å¼2æ‡‰è©²åªæœ‰2å€‹å¿…å¡«é …ç›®éŒ¯èª¤")

        # æª¢æŸ¥å…·é«”çš„å¿…å¡«é …ç›®
        error_text = "\n".join(required_errors)
        self.assertIn("ç›®æ¨™æ¬„ä½", error_text)
        self.assertIn("è¨“ç·´è³‡æ–™æª”æ¡ˆ", error_text)
        self.assertNotIn("æ¨¡å‹è¼¸å‡ºè³‡æ–™å¤¾", error_text)
        self.assertNotIn("æ¨¡å‹æª”æ¡ˆåç¨±", error_text)

        print("âœ… æ¨¡å¼2å¿…å¡«åƒæ•¸é©—è­‰æ­£ç¢º")

    def test_mode3_required_parameters(self):
        """æ¸¬è©¦æ¨¡å¼3çš„å¿…å¡«åƒæ•¸é©—è­‰"""
        print("=== æ¸¬è©¦æ¨¡å¼3å¿…å¡«åƒæ•¸é©—è­‰ ===")

        app = self.mock_apps["3"]
        validator = ParameterValidator(app)

        # æ¸¬è©¦ç©ºåƒæ•¸
        errors = validator.validate_all_parameters()
        required_errors = [e for e in errors if "å¿…å¡«é …ç›®" in e]

        self.assertEqual(len(required_errors), 4, "æ¨¡å¼3æ‡‰è©²æœ‰4å€‹å¿…å¡«é …ç›®éŒ¯èª¤")

        # æª¢æŸ¥å…·é«”çš„å¿…å¡«é …ç›®
        error_text = "\n".join(required_errors)
        self.assertIn("ç›®æ¨™æ¬„ä½", error_text)
        self.assertIn("è¨“ç·´è³‡æ–™æª”æ¡ˆ", error_text)
        self.assertIn("æ¨¡å‹è¼¸å‡ºè³‡æ–™å¤¾", error_text)
        self.assertIn("æ¨¡å‹æª”æ¡ˆåç¨±", error_text)

        print("âœ… æ¨¡å¼3å¿…å¡«åƒæ•¸é©—è­‰æ­£ç¢º")

    def test_hyperparameter_tuning_validation(self):
        """æ¸¬è©¦è¶…åƒæ•¸èª¿å„ªç›¸é—œçš„é©—è­‰é‚è¼¯"""
        print("=== æ¸¬è©¦è¶…åƒæ•¸èª¿å„ªåƒæ•¸é©—è­‰ ===")

        for mode in ["1", "2", "3"]:
            app = self.mock_apps[mode]

            # è¨­å®šæœ‰æ•ˆçš„åŸºæœ¬åƒæ•¸
            app.target_column.set("is_recommended")
            app.train_data_path.set("traning_data/train_data(top20).csv")
            app.model_output_folder.set("output_models")
            app.model_filename.set("model.bin")

            # è¨­å®šç„¡æ•ˆçš„è¶…åƒæ•¸èª¿å„ªåƒæ•¸
            app.cv_folds.set(0)  # ç„¡æ•ˆå€¼
            app.importance_n_repeats.set(-1)  # ç„¡æ•ˆå€¼

            validator = ParameterValidator(app)
            errors = validator.validate_all_parameters()

            tuning_errors = [e for e in errors if any(keyword in e for keyword in
                                                      ["äº¤å‰é©—è­‰", "ç‰¹å¾µé‡è¦æ€§"])]

            if mode == "1":
                self.assertEqual(len(tuning_errors), 0,
                                 "æ¨¡å¼1ä¸æ‡‰è©²é©—è­‰è¶…åƒæ•¸èª¿å„ªåƒæ•¸")
                print(f"  æ¨¡å¼{mode}: âœ… æ­£ç¢ºä¸é©—è­‰èª¿å„ªåƒæ•¸")
            else:
                self.assertGreater(len(tuning_errors), 0,
                                   f"æ¨¡å¼{mode}æ‡‰è©²é©—è­‰è¶…åƒæ•¸èª¿å„ªåƒæ•¸")
                print(f"  æ¨¡å¼{mode}: âœ… æ­£ç¢ºé©—è­‰èª¿å„ªåƒæ•¸ ({len(tuning_errors)} å€‹éŒ¯èª¤)")

    def test_mode_specific_parameter_isolation(self):
        """æ¸¬è©¦å„æ¨¡å¼åƒæ•¸é©—è­‰çš„éš”é›¢æ€§"""
        print("=== æ¸¬è©¦åƒæ•¸é©—è­‰éš”é›¢æ€§ ===")

        # æ¸¬è©¦æ¨¡å¼2ä¸å—æ¨¡å‹è¼¸å‡ºåƒæ•¸å½±éŸ¿
        app_mode2 = self.mock_apps["2"]
        app_mode2.target_column.set("is_recommended")
        app_mode2.train_data_path.set("traning_data/train_data(top20).csv")
        # æ•…æ„ä¸è¨­å®šæ¨¡å‹è¼¸å‡ºåƒæ•¸

        validator = ParameterValidator(app_mode2)
        errors = validator.validate_all_parameters()

        # æ¨¡å¼2æ‡‰è©²æ²’æœ‰ä»»ä½•éŒ¯èª¤ï¼Œå› ç‚ºå®ƒä¸éœ€è¦æ¨¡å‹è¼¸å‡ºåƒæ•¸
        self.assertEqual(len(errors), 0, "æ¨¡å¼2è¨­å®šå¿…è¦åƒæ•¸å¾Œæ‡‰è©²æ²’æœ‰éŒ¯èª¤")
        print("âœ… æ¨¡å¼2åƒæ•¸éš”é›¢æ€§æ­£ç¢º")

        # æ¸¬è©¦æ¨¡å¼1å—æ¨¡å‹è¼¸å‡ºåƒæ•¸å½±éŸ¿
        app_mode1 = self.mock_apps["1"]
        app_mode1.target_column.set("is_recommended")
        app_mode1.train_data_path.set("traning_data/train_data(top20).csv")
        # æ•…æ„ä¸è¨­å®šæ¨¡å‹è¼¸å‡ºåƒæ•¸

        validator = ParameterValidator(app_mode1)
        errors = validator.validate_all_parameters()

        model_output_errors = [e for e in errors if "æ¨¡å‹" in e and "å¿…å¡«" in e]
        self.assertGreater(len(model_output_errors), 0, "æ¨¡å¼1æ‡‰è©²è¦æ±‚æ¨¡å‹è¼¸å‡ºåƒæ•¸")
        print("âœ… æ¨¡å¼1åƒæ•¸è¦æ±‚æ­£ç¢º")

    def test_parameter_conflict_detection_across_modes(self):
        """æ¸¬è©¦æ‰€æœ‰æ¨¡å¼éƒ½æ­£ç¢ºæª¢æ¸¬åƒæ•¸è¡çª"""
        print("=== æ¸¬è©¦è·¨æ¨¡å¼åƒæ•¸è¡çªæª¢æ¸¬ ===")

        for mode in ["1", "2", "3"]:
            app = self.mock_apps[mode]

            # è¨­å®šè¡çªçš„åƒæ•¸
            app.target_column.set("is_recommended")
            app.exclude_columns.set("rating,is_recommended,price")
            app.train_data_path.set("traning_data/train_data(top20).csv")
            app.model_output_folder.set("output_models")
            app.model_filename.set("model.bin")

            validator = ParameterValidator(app)
            errors = validator.validate_all_parameters()

            conflict_errors = [e for e in errors if "åƒæ•¸è¡çª" in e]
            self.assertGreater(len(conflict_errors), 0,
                               f"æ¨¡å¼{mode}æ‡‰è©²æª¢æ¸¬åˆ°åƒæ•¸è¡çª")
            print(f"  æ¨¡å¼{mode}: âœ… æ­£ç¢ºæª¢æ¸¬åƒæ•¸è¡çª")


def test_execution_mode_validation():
    """ä¸»è¦æ¸¬è©¦å‡½å¼"""
    print("ğŸš€ é–‹å§‹æ¸¬è©¦ä¸‰ç¨®åŸ·è¡Œæ¨¡å¼çš„åƒæ•¸é©—è­‰...")
    unittest.main(verbosity=2)


if __name__ == '__main__':
    unittest.main(verbosity=2)
